\label{key}\section{Introduction}
State of the art asymmetric cryptosystems compared with usage of sufficient long Keys are considered as "safe" on current computer systems. This changes immediately when quantum computers enter the scene. But why is this the case? In this paper I briefly present the main differences between normal computer systems and quantum computers and why state of the art cryptosystems like RSA and ECC are not save in the quantum computation world. In addition to that I, present a extensive introduction to the McEliece Cryptosystem explain the strengths and downsides of the algorithm and give some details about the developments of codes which can be used for the system.

\label{conv_comp_vs_quan_comp}
Since decades computer systems have been seen as digital circuits based on the rules of physics. A bit in this systems is seen as a value of voltage. If this value is above a certain, level the binary representation is one, otherwise it's zero. 
On quantum computers, information is handled in binary, too. The main difference between the bit representation and the quantum bit representation (so called qubit) is that in addition to the two states zero and one, there are several more states which one qubit can reach due to a superposition. This means that a qubit is not either in state one or in state zero but in theory can have an arbitrary number of different states. All these states can appear with an certain probability. Each of these states is able to compute one possibility of a NP hard problem. 

Due to this fact, quantum computers are able to solve problems which are NP hard in sense of complexity theory much faster than traditional computers. 

One of these computational problems, which are traditionally hard to solve, is the prime factorization problem which is in serveral variations the core of many public key algorithms such as the RSA cryptosystem. 
The second main category, the elliptic curves cryptography is even more affected by this problem as the logarithm of a finite field of an elliptic curve can be computed efficiently and can be broken in less time compared to the RSA algorithm  because of lower keysizes. 
In conclusion, there is no established asymmetric cryptoscheme which is post quantum resistant. Therefore completely different approaches are needed. 

\subsection*{Outline}
In \autoref{post_quantum}, some background information regarding post quantum cryptography is given. With help of Shors algorithm the principle of quantum cryptography is described more precisely. \autoref{mceliece} takes a detailed look into the McEliece crypto system followed by the Niederreiter cryptosystem in \todo{make label}. 

%1. State of the art cryptography vs. Post quantum cryptography schreiben
%2. Shoor überarbeiten
%3. McEliece Abstrakte Idee / Grundlagen / Algorithmus → Formeln
%4. Struktur review
%	a. Goppa codes nach McEliece
%	b. Niederreiter
%	c. Weitere Codes → Reed Solomon ... ?
%5. Niederreiter  
%6. Digital Signatures
%7. Eigenschaften der Goppacodes
%8. Attacker models (abstrakt)
%9. Wild McEliece
%10. Weitere Themen evaluieren. 
%11. Struktur review 2
%	b. Attacker models tiefergehend
%	1. Padding

\section{Post quantum cryptography}
\label{post_quantum}
\todo[inline]{What makes cryptosystems strong?}
In order to reach post quantum proof cryptographic algorithms, some fundamentally different then the established ones are required. Shors Algorithm in \ref{shoor} reveals that cryptography based on integer factorization or the discrete logarithm problem is no longer an issue for quantum computers. Therefore other algorithm types are needed which are shortly described in \ref{pqalgcand}.   

\subsection{Shors Algorithm}
\label{shoor}
Peter Shor presented in 1994 an algorithm which is able to factorize a composite number n into its prime factors. This algorithm is especially designed for quantum computers on which it only needs $log n$ qubits and has a runtime of $O((log n)^3)$ for finding a non trivial factor of n. 

\subsubsection{Procedure}
\todo{review section --> more abstract}
Shors algorithm divides into a classical part which can be executed on a conventional computer and a quantum part which has to be executed on a quantum computer in order tp perform the computation efficiently. 

The basic idea is that the classical part reduces the problem while the quantum part finds the order of the group in which n is. 
\paragraph{Classical part}
The classical part of the algorithm mainly contains computation of the greatest common devisor of a randomly selected number lower than $n$ and $n$ itself. Now we need to compute the order $r$ of $x$ which is where the quantum part emerges. The classical part is executed in a loop while the order $r$ is odd or $x$ to the power of $r/2$ is equivalent to $-1 mod n$  \todo{why?} \todo{make formula beautiful}
If this is not the case the gcd of $x^(r / 2) -1$ n is computed.
\paragraph{Quantum part}
At first there a $q$ is determined which is a power of $2$ and lies between $n2$ and $2 n2$. 
A random $a$ which is lower than n is selected and the input quantum register is initialized with all states of $a mod q$. \todo{formel}
The output quantum register is initialized with all states of xa(mod n). 

A quantum Fourier transformation is computed on the input register. 

The result values are gathered from the input register. 

\todo{vllt. doch abstrakter}
To sum up Shors algorithm with support of a quantum computer is able to find the period of a prime in polynomial time. 

With this algorithm all cryptography based on prime factorization can be broken by a quantum computer in polynomial time. 

\subsubsection{Complexity of Shors Algorithm} \todo[inline]{on normal pc and on quantum pc}
Considering the complexity of Shors algorithm on quantum computers, it's easy to see that there is a complexity of $O(log n)$ which is in the class of BQP. This class is comparable to the class BPP on conventional computers. In this regard the an state of the art RSA key of 2048 bit length can be broken in xxxx\todo{kann solch eine Aussage treffen?}. In the case of elliptic curves cryptosystem it's even worse. Due to the principal of the algorithm ECC is gathering more security per bit keylength on a conventional computer and only has keys up to 512 bit. This is not the case on quantum computer which makes them even easier to break compared to RSA keys.
\todo[inline]{BQP / BPP mit reinnehmen??? Könnte interessant sein. Unter oder neben Shor?}

\subsection{Candidates for post quantum cryptography}
\label{pqalgcand}
In this section a short overview over promising state of the art post quantum cryptoschemes based on \cite{bernstein2009introduction} is given. 
\paragraph*{Lattice-based cryptography}
One of the most studied types of algorithm is the lattice based cryptography which exists in several variants. Algorithm works on a lattice over a n-dimensional finite Euclidian field $L$ with a strong periodicity property. A set of vectors provides the basis of $L$ in the way that every element is uniquely represented. The cryptographic problem is to find the closest vector to a given lattice point e.g. by adding an error vector\cite{bernstein2009introduction}\cite{wiki:lattice}.
\paragraph*{Multivariate cryptography}
Multivariate cryptography is based on a multivariate polynomials over a finite field $F$ which are defined over both a ground and an extension field. In the case of solving systems they are NP-complete and due to this fact a candidate for post quantum cryptography. They are topic of studies for a long time and  are promising especially for signature schemes\cite{bernstein2009introduction}\cite{wiki:multi}.

\paragraph*{Hash-based cryptography}
Hash-based algorithms such as Lamport-\cite{wiki:lamportsig} and the Merkle\cite{wiki:merklesig} signature scheme are based on strong hash functions but have the drawback that only a limited count signatures can be created per key. The algorithm reduces the one time signature to an hash value using a hash function\cite{bernstein2009introduction}.

\paragraph*{Code-based cryptography}
The forth group, the code based algorithms, are based on error-correcting codes. First investigations have been developed by Robert McEliece using random Goppa codes\cite{wiki:bingoppa}. This paper deals with the properies of McEliece- and the related Niederreiter cryptosystem\cite{bernstein2009introduction}\cite{wiki:niederreither}.


The raw overview of some state of the art algorithms shows different aproaches but in general it could be said that a much higher computational effort has to be taken to achieve a strong system compared to the traditional ones. 

\section{The McEliece cryptosystem}
\label{mceliece}
Back in 1978, Robert McEliece suggested an asymmetric quantum resistant cryptosystem based on the theory of algebraic codes. He selected binary Goppa codes with the property irreducibility as base a for the cryptosystem\cite{mceliece1978public}. The chosen code $C$ has a length of $n = 2^m$ and a dimension of $k >= n - tm$. These codes are able to correct any pattern of $t$ or fewer errors. 
For each of this codes, there exists an irreducible polynom of degree $t$ over $GF(2^m)$. 
The main reason for McEliece to select this setup is that there exists an fast algorithm to decode these codes\cite{mceliece2002theory}.

\subsection*{Key generation}
For the key-generation, a $n$ and $t$ with above mentioned properties are picked. Additionally, an irreducible polynomial of degree $t$ over $GF(2^m)$  is selected randomly. The probability that this selection leads to an irreducible polynomial is $1/t$ and there is an efficient algorithm to prove this\cite{berlekamp1968algebraic}.
In the next step a generator matrix $G$ which is of size $n \times k$ is produced. This can be transformed into canonical form. 

Now the information of $G$ has to be camouflaged. Therefore a random dense $k \times k$ matrix $S$ which is nonsingular and a random $n \times n$ permutation matrix $P$ is selected. Both of them are multiplied to $G' = SGP$. Due to the matrix multiplication properties, the linear code generated by $G'$ has the same rate and distance like $G$. $G'$ is the public generator matrix and is sent to the encrypting entity. 
\newline
\newline
The following encryption algorithm is published so that the encrypting entity can use it. 

\subsection*{Encryption}
First of all, the message $m$ which is to be encrypted has to be devided into $k$-bit blocks. The public key encryption is performed by $x = uG' + z$ with $u$ beeing one of such a $k$-bit block. In this case, $z$ is a randomly generated vector with length $n$ and weight $t$ \footnote{The weight of an vector is defined as Hamming weight.}.
\newline
\newline
$x$ is the encrypted message which is transmitted to the private key owner who can decrypt the message block $u$ as follows. 

%To encrypt a message m the calculation $m * \hat{G} + z$ is performed. In this case z is an random matrix $ \in {0,1}^{n}$ with a Hammingweight of d.
\subsection*{Decryption}
The decryption of one block $x$ starts with computing $x' = xP^{-1}$ with $P^{-1}$ as inverse of the permutation matrix $P$. 

%$x'$ is now a codeword in the Goppacode $C$ which was choosen during key generation. 
With an error correcting algorithm for the code $C$, the codeword $u'$ next to $x'$, is calculated. 
To get a plaintext message block the computation $u = u'S^{-1}$ is performed\cite{wiki:mcelice}\cite{mceliece1978public}.
As an efficient method for calculating the error corrections of $x'$ to $u'$, McEliece suggests the algorithm of Patterson\cite{patterson1975algebraic}. Refer also to \autoref{decoding_properrties}. 
% To decrypt the message we need the error correction properties of the codes. At first  $c' = cP^{-1}$ is computed. Now we computed the next codeword c'' from c' and therefrom the next message c'''. With $m = c''' * S^{-1}$ we derive the message from the ciphertext. 


\subsection*{Correctness}
Assuming that $P$ is a permutation matrix and randomvector $z$ with length $n$ and weight $t$, it is obvious that $zP^{-1}$ has a weight of $t$ or less.
As discussed, the computation is $c' = c P - 1 = uG'P^{-1} + zP^{-1} = uSG + z P^{-1}$.
The chosen Goppa code $C$ is designed to correct up to $t$ errors. On the other hand, $mSG$ has a maximum distance from $cP^{-1}$ of $t$. This leads to the fact that the correct code $mS$ is determined by the algorithm. 
To obtain the message block $u$ from $uS$, we can easily multiply the inverse $u = uSS^{-1}$\cite{wiki:mcelice}.

\subsection*{Security properties}
The security of the presented scheme refers on the one hand to the basics of learning with errors principle. More precise the hypothesis of \textit{Learning Parity with Noise}\cite{pietrzak2012cryptography}. \todo{ausweiten --> paper}
On the other hand it refers to the hypothesis that the generator matrix $G$ is indistinguishable from any other $k \times n$-matrix. This leads to the property of a trapdoor function.

%Unter der Learning-Parity-with-Noise-Annahme und der Annahme, dass G^ ununterscheidbar von zufällig k × n Matrizen ist, besitzt das Verfahren die Einwegeigenschaft.

\section{The Niederreiter cryptosystem in comparison to McEliece}
\label{niederreither}
The Niederreiter cryptosystem is highly comparable to the McEliece cryptosystem due to the fact that it is following the same basic idea. 

Niederreiter designed his $(n, k, 2t + 1)$ linear code $C$ over a Galois field too. In contrast to McEliece the code size does not have to be a power of 2 instead of an arbitary integer $GF(q)$. 
%MCE

Another difference to McEliece is the usage of a $(n - k) \times (n)$ parity check matrix $H$ instead of  the generator matrix $G$. %pc matrix
The nonsigular $(n - k) \times (n - k)$ matrix $M$ is defined sightly different from McEliece ($k \times k$)\todo{why?}.% ani non singular
%permutation matrix == MCE
The permutation matrix $P$, an arbitrary $n \times n$ matrix, is exactly defined in the same way compared to McEliece.
The private key is then defined as $M, H and P$ the public key consists of $H' = MHP$ and the hamming weight $t$ which is quite the same compared to McEliece.
The messages in the system of Niederreiter have to be $n$ dimensional vectors over $GF(q)$ and they must have a hamming weight of $t$. This is an important fact issuing the signature creation in \autoref{signature}\todo{why?}.
Encryption with Niederreiter is performed with $z = yH'^T$ which again is comparable with the encryption operation $x = uG' + z$ in McEliece. The error vector $z$ is not needed here because this is already represented in ... \todo[inline]{why is it not needed to add the error vector here?}. The ciphertext has only n - k bit dimension compared to n bit in McEliece. 
The decryption is computed firstly with $(yP^T)H^T = z(M^T)^(-1)$ Then $H$ is eliminated by an error correction algorithm which leads to $(yP)^T$ which can easily computed to the plaintext $y$. \cite{sendrier2011niederreiter}\cite{li1994equivalence}\cite{niederreiter1986knapsack}

%This is a knapsack-type cryptosystem which employs an(n, k , 2t + 1)linear code C over GF(q). Let E be an ( n - k) x nparity check matrix of C, M any ( n - k ) x (n - k) nonsingularmatrix, and P any n x n permutation matrix, all over GF(q).
%Private Key: E , M , and P.
%Public Key: H‘ = M H P and t .
%Messages: n dimensional vectors J over GF(q) with weight t.
%Encryption: z = #HIT. z, the ciphertext of dimension n - k.
%Decryption: Since z = J ( M H P ) ~ z(MT)-’ , = (@“)BT.
%Use a fast decoding algorithm for C to find g$ and thus g.Niederreiter [3] cryptanalyzed his system and mentioned twoexample systems, one using a binary concatenated (104,24, 31) codeand the other using a (30, 12, 19) ReedSolomon code over GF(31).The examples were later verified as insecure by Brickell and Odlyzko


% The Niederreiter variant - equivalent on a security point of view [8] - uses a syndrome (see below) as ciphertext, and the message is an error pattern instead of a codeword (see Table 1).


\section{Signing with Niederreiter}
\label{signature}
Besides en- and decryption signatures building and verification is an common requirement to an asymmetric cryptoscheme. In state of the art algorithms principal is quite simple: The message to be signed is \textit{de}crypted with a given public key. The verifier \textit{en}crypts the message with his private key an compares the result with the message. 

In case of McEliece this isn't so easy because it is not possible to decrypt (= sign) an message before encrypting (= verify) it. More precise the process of signing produces a syndrom whose \todo{dessen} error pattern is bigger than the error correcting property $t$. In fact it is hard to create a ciphertext that fits to the error correcting properties of the encryption without using it. 


Compared to ecnryption and decryption siging and verifying is much harder to realise with McEliece. Just in 2006 a digital signature scheme were presented by Courtois et. al. 
The problem with signing a given hash value n is that generally it is longer than the decoding capacity t of the used code. More general one can say that it is difficult to generate a random ciphertext without using the encyption algorithm. \cite{courtois2001achieve}


\subsubsection*{Complete decoding}
One possible solution would be to use complete decoding. Therefore not only the words within the radius of $t$ can be decoded but all words laying in the code space. In other words with complete decoding we can find an erro pattern to any given syndrome as long as it is in the code space. This means that we have to add a $\delta$ with random collumns from the parity check matrix\todo{why pc-matrix}to $t$. The decoding works exactly when all of the $\delta$-columns fit to an error position because then the syndrome will fit to an word of weight $t$. Else we have to add another $\delta$ to $t$ and try again.

From this properties we now can construct a digital signature scheme: We have to select a $\delta$ which is small enough to get an usable key size but on the other hand has a good security. 

For achieving a small $\delta$ the code has to be selected carefully in the way that it has to have a high density of decodable syndromes. This makes sure that the $\delta$ is kept small because the probability of finding a fitting one is high. For building up a signature the signer now takes a syndrome and hashes it together with the document. This is tried with an modified document (possibly with some kind of padding) as long as he gets a decodable syndrome. \cite{courtois2001achieve}

\todo{find parameter for this.}

\section{Codes can be used for MCElice and Niederreiter} \todo{both the same?}
As described in the last sections the McEliece cryptosystem is based on the properties error correcting codes. In principle many codes with a good decoding algorithm could be used for the cryptosystem. However there are some properties which leads to higher security than others. In this section a short introduction to the basics of error correcting codes is presented followed by a deeper view on Goppa codes which are suggested by McEliece and are still the best choice.

\todo{what makes a code usable}
\subsection*{Principles of error correcting codes}
As the name states the basic purpose of error correcting code is to detect and correct errors which occur during the transition of messages. Therefore additional information is added to the message by the sender. Knowing the encoding algorithm receiver is able to decode the original word, detect errors and correct them with help of the additional information and the corresponding decode algorithm. 
\todo{start earlier: principles of error correction}
\subsubsection*{Binary} Due to the fact that almost all transition is performed by computers and is in binary the explanations in following are focussed on the binary case too. Never the less all of them hold in the general case. 

%creating a code
For generating these additional information a vector with binary indexes $V = {[c1 c2...cn] |ci \in {0, 1}}$ is created as set of $\mathbbm{F}_2$. Further the addition is defined componend-wise modulo 2, which also can be seen as XOR operation. With this definition $V$ covers all $n$-tuples in $\mathbbm{F}_2$. 
Having this, a binary, linear $[n, k]$ code can be defined as subspace of $\mathbbm{F}_2$ in which the $n$ is the length and the $k$ is the dimension. A codeword is represented as vector in the code.
 
%Let V = {[c1 c2 · · · cn] |ci ∈ {0, 1}}, where addition in V is component-wiseaddition (mod 2). So V is the set of all n-tuples of elements of F2 . Then we define an [n, k] binary∼ nlinear code as a k-dimensional subspace of V = F2 . The parameters n and k are called the lengthand dimension, respectively, of the [n, k] binary linear code. Throughout the remainder of thispaper we will use the word code to mean a binary linear code. A codeword is any vector in the code.

\subsubsection*{Hamming distance} Another important variable for error correcting codes is the hamming weight or more precise the minimum weight $d$ of a vector. The weight is simply defined as count of non-zero positions in the vector. Furthermore the minimum weight is defined as minimum is the smallest weight of a codeword under the condition that it is not zero. 
A $[n, k, d]$ code can correct $t =  \lfloor \cfrac{d - 1}{2}\rfloor$ errors. 

\subsection*{Goppa Codes}
\label{goppa}
Goppa codes follow the principals described previously. In this section highlights the special basics (of Goppa codes) regarding the McEliece cryptosystem. Firstly some important basic properties and terminologies are presented. In the second part an example is discussed\todo{DO SO}. 

\subsubsection*{Linearity}
Goppa Codes are linear codes. This is not a conincidence because non-linear codes do not provide an efficient decoding algorithm like Pattersons algorithm for linear codes does\cite{patterson1975algebraic}. In \cite{zeng2014nonlinear} some remarkable results to achieve more efficiency are stated but also it is mentioned, that nonlinear codes are not able to achieve more comparable efficiency. 
However most of the popular codes are linear ones this condition does not rule out so many codes.

\subsubsection*{Irreducibility} 
y
\todo[inline]{das könnte auch in ECC, da alle eccs durch polynomer erzeugt werden können und die irreduzibel sein müssen oder?}
To achieve a finite field with usable properties for a Goppa code the generator polynomial has to bew irreducible. This means that for a polynomial $p$ over a finite field $GF(p^m)$ there exists no polynomial $p' \in GF(p^m)$ of lower degree which divides $p$.
 

\subsubsection*{Efficency(?)} To keep the keyspace as efficient as possible the factor between the matrix dimensions $n$, $k$ and the error correction range $t$ (respectively the minimum hamming distance $d$) should be as high as possible. The most important point is that Goppa Codes are a suspect of research for years so the probability of is really low. 



%Basic properties: 
%Binary OK
%Irreduceable OK
%seperatable
%syndrom
%good decoding properties

\section{Example of generating a Goppa Code and the McEliece cryptosytem}
In \ref{mceliece} the threoretical the aspects of the McEliece cryptsystem were described and in \ref{goppa} the basics of generating a code were highlighted. In this section an example of of both is presented. 
Projecting these parameters onto the MCeliece cryptosystem ... \todo{continue}

% Endweder
% Usability & Keylengths
% -> Keylengths
% Ausblick
% oder 
% Ausblick / Further topics
% -> Usability
% -> Keylengths
% -> ...

\todo[inline]{ab hier eher ausblick --> further topics}
\section{Further topics}
To get an elaborated overview over all facets of the McEliece cryptosystem many more topics have to be issued which are not covered in detail in the article. In this sections some of the most important topics are described slightly. 

\todo[inline]{EXPERIMENTEL: 
	Erst attacker models, dann real world security attacks dann keylangth --> alle unter ein überschrift.}
\todo[inline]{CPA / CCA1 / CCA2 }
\todo{ausweiten aber niedrige prio}
\subsection{Resistency against various attacker models}
foo
\paragraph{Achieving chosen cipher text resistance}

Like the RSA cryptosystem in the original idea is not resistant against chosen cipher attacks. 

This means if an attacker has access to an oracle which is able to decrypt an given ciphertext without knowing the key and the attacker is permitted to give all ciphertexts except the one he is asked to crack to the oracle he is not able to gather any reasonable information from the oracle. This goes over several iterations and is called CCA2-attack. In literature resistance against this attack is described as IND-CCA2. 


Like the RSA cryptosystem uses padding standards such as PSS for signing and OAEP for encrypting to achieve IND-CCA2 the MECS has some padding schemes as well.


\subsection{Practical security attacks}
\label{keylength}
Every cryptosystem with certain parameters has to hold against attacks to be considered as safe. These attacks can be very different for every cipher suite. An easy approach to break a system is the brute force attack in which every possible private key is tried out to decrypt the message. 
For the McEliece cryptosystem there are much better attack such as structural and information set decoding attacks. 
Structural attack ...

Information set decoding is the most promising method of breaking McEliece so far. The idea behind this is to find a set of error free coordinates in a codeword. If the corresponding columns in the generator matrix form a invertible submatrix the information from original message word can be obtained easily. 

 If there is a (n,k,t) code of a generator $G$ and a ciphertext $c = uG + e \in  \mathbbm{F}^n_2$. An attacker now randomly selects a subset (the information set) $I \subset 1, ..., n$ of size k beeing linearly independent. Now an ``masking'' function $\delta$ is needed which projects $c$ on $I$ such as $\delta(c)_{i \in I} = (uG)_{i\in I}+e_{i\in I}$
 
 If this projection leads to a error free result (also $weight(e) = 0$) we can obain the message word from the code by $\delta(r)* \delta(G)^{-1} = \delta(uG)^{-1}*\delta(G)^{-1} = u$
 Note that this is not the commen case an typically needs many iterations.
 
  %an generates \hat{G}_I from it and the public key. He now hopes that the generated matrix is invertible so that he can obtain the information from original message word.  contains no errors 


\subsection{Recommended parameters}
In \cite{niebuhr2012selecting} Niebuhr et al computed some key length based on the Model of Lenstra and Verheul. They motivated that keylength have to grow with speedup of computer hardware and concluded state of the art keylenths of traditional algorithms which is ported to the McEliece Cryptosystem by Niebuhr et al. Based on the best known attack of Sendrier at al, which is able to break the original parameters ($n = 1024$, $k = 524$, $t = 50$) with a minimal binary work factor of $2^59,9$ operations \cite{finiasz2009security}, they presented the following values as secure parameters until the year 2050: $n = 2804$, $k = 2048$ and $t = 66$. A public key generated has a size of about 189 kilobyte. This is almost 400 times more keyspace than a RSA key and almost 3000 times more than an eliptic curves key which promises safety until 2050 too, but only on conventional computers. The key size issue is decribed a liitle more into detail in \ref{space_efficiency}.


\section{Open Challanges}
From the practical point of view the presented schemes more sketches of one way trapdoor functions than fully filled crypto schemes. These trapdoors can be used in many different ways to generate a ``standard'' crypto scheme maybe as replacement for RSA- and ECC-cryptography, but this has to be done carefully. In this section a short summary of the challenges are such as usability properties, computation time and key sizes is presented \todo{followed by some variants of the scheme. ???} These three topics are highly connected to each other. 

\subsection*{Confidence}
\label{confidence}
One of the most delicate tasks when putting an abstract algorithm into a real crypto application is to achieve confidence into\todo{from?} it. Many pitfalls like wrong codes or parameters and weak padding algorithms have to discovered by cryptoanalysts to increase the confidence level. However both, the RSA and the McElice where suggested over 35 years ago, the development on RSA was pushed forward much faster because of the adavantage of a lower keyspace. This development has to be done in the next years for McEliece to get over issues discoverd in the past and achieve an optimized reliable system. 

\subsection*{Efficiency}
In most cases the abstract term ``efficiency'' is divided in time- and space efficiency. Daniel J. Bernstein made \cite{bernstein2009introduction} a short comparison with the RSA which should be summed up here shortly. 

\paragraph*{Time effciency} is seen as time needed to perform the key generation, encryption and decryption. More precisely the different ``operations'' executed and their complexity are reviewed. 

Compared to RSA the McEliece is quite time efficient. As described in \autoref{mceliece} the main computations are matrix operations in the binary field, which can be broken down to highly efficient additions, multiplications, shifts etc. for which no special hardware requirements are needed\cite{bernstein2009introduction}.
\todo{ggf gegensatz zu rsa beschreiben}

\paragraph*{Space efficiency} \label{space_efficiency} refers to the storage needed to be provided in order to execute the algorithm. In our case the bytes needed to transmit the public key to the encryption entity or signer additionally is an important topic. 

Is most important drawback is the key sizes to be transferred. With $n$ denoted as code length respectively key length McEliece needs about $b^2*(lg b)^2$ bits whereas the RSA only needs $(~0,16)*b^3/(lg b)^2$ bits. One might notice the higher exponent in RSA formular which leads to an faster growing key space if the security level goes to $\infty$ but the break even point \todo{heißt das so??} is fare beyond practical relevant key sizes. For keys assumed as secure in these days the McEliece key is size is about 10 times higher than RSA\footnote{Assuming unoptimized but optimal algorithms without attention of quantum computers.}. Refer also to \autoref{keylength} for a deeper view on keylenght with different parameters.

\paragraph*{Usability}
The last important point is to provide an usable interface for the cryptosystem. New crypto protocols can only be successful if many applications in different domains are supporting them. Therefore padding scheme like addressed in \autoref{confidence} has to be provided and standardised to make sure that different software systems on different platforms can speak to each other. 

\subsection*{Optimizing McEliece} %oder optimizing MCE
Due to the fact that the McEliece cryptosystem was published over 30 years ago and still is one of the most promising post quantum security algorithms many variants came up. Many of the previous described issues could be solved by reducing the key space. In conclusion this means optimising on the used codes. Many codes like Reed-Solomon codes, quasi dyadic codes, and variants of Goppa codes have been tried, seen as secure and been broken a little later. 

\subsubsection*{Wild McEliece}
The most promising optimization is idea of using ``wild'' Goppa codes\cite{bernstein2010wild}. Wild Goppa codes are codes which are no longer over a field $F_2$ but on $F_q$ with $q$ as small prime number. These approach was broken by generating the squeare code of the public key and revealing information of the original private key, because the square product lines are not equally distributed compared to the binary code. In \cite{yang2011post} the approach wild McEliece idea is improved with an ``incognito'' variant. As the name says the wild Goppa codes are hidden by multiplying a extra factor $f$ to the code and by using only special codes described in \cite{berger2005mask}. This topic and the question if squaring technique is still considerable for the incognito version, is still a topic of research. 



\section{Conclusion}
...
